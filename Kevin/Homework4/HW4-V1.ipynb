{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevinli/src/Python_For_Data_Science/Kevin/Homework4\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m50_categories\u001b[m\u001b[m\r\n",
      "50_categories.tar.gz\r\n",
      "HW4-V1.ipynb\r\n",
      "hw_4-machine-learning-parallel-strawman.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import cPickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Image processing and feature extraction\n",
    "\n",
    "To compute features for training, run:\n",
    "\n",
    "`python image_processing.py`.\n",
    "\n",
    "This script assumes there is a directory called `50_categories`, which\n",
    "contains subdirectories named by category, and then within each\n",
    "subdirectory the actual images.\n",
    "\n",
    "Running this script from the command line will load the images,\n",
    "perform some basic preprocessing (equalizing, scaling, etc.), and\n",
    "compute features. It will then save the feature array to a file called\n",
    "`image_dataset.npy`, and the categories to a file called\n",
    "`image_categories.npy` (these are both numpy files, and can be loaded\n",
    "by calling `np.load`).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# built-in\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "# external\n",
    "import numpy as np\n",
    "import skimage.exposure\n",
    "import skimage.feature\n",
    "import skimage.filter\n",
    "import skimage.filter.rank\n",
    "import skimage.io\n",
    "import skimage.morphology\n",
    "\n",
    "\n",
    "def load_image(img_path, n=400):\n",
    "    \"\"\"Load an image from file, and perform minimal processing on it to\n",
    "    prepare it for feature extraction.\n",
    "\n",
    "    Specifically, this function does the following operations:\n",
    "\n",
    "        1) Load image\n",
    "        2) Convert to RGB if grayscale\n",
    "        3) Equalize histograms\n",
    "        4) Denoise\n",
    "        5) Resize\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : string\n",
    "        The path to the image\n",
    "    n : int (optional)\n",
    "        The size to scale the largest dimension to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img : numpy.ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    # load the image from file\n",
    "    img = skimage.io.imread(img_path).astype('f8')\n",
    "    # make sure it has three channels\n",
    "    if img.ndim == 2:\n",
    "        img = img[:, :, None] * np.ones(img.shape + (3,))\n",
    "\n",
    "    # equalize histograms\n",
    "    img = skimage.exposure.equalize_hist(img)\n",
    "    # reduce noise\n",
    "    img = skimage.filter.denoise_bilateral(img, 3, 0.1)\n",
    "\n",
    "    # scale largest dimension to be of size n\n",
    "    shape = img.shape[:2]\n",
    "    scale = float(n) / max(shape)\n",
    "    img = skimage.transform.rescale(img, scale)\n",
    "    shape = img.shape[:2]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"Extract a vector of features from an image. The feature vector is\n",
    "    flat, but has the following components:\n",
    "\n",
    "        1) Mean of R, G, and B channels\n",
    "        2) Covariance between R, G, and B channels\n",
    "        3) Summary statistics of image entropy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : numpy.ndarray\n",
    "        The image to extract features from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_vec : numpy.ndarray\n",
    "        One-dimensional numpy array of features\n",
    "\n",
    "    \"\"\"\n",
    "    RGB = img.reshape((-1, 3)).T\n",
    "\n",
    "    # mean of each channel\n",
    "    mean = np.mean(RGB, axis=1)\n",
    "    # median of each channel\n",
    "    median = np.median(RGB, axis=1)\n",
    "    # covariance between channels\n",
    "    cov = np.cov(RGB).ravel()\n",
    "    # (normalized) entropy of the grayscale image\n",
    "    entropy = skimage.filter.rank.entropy(\n",
    "        np.mean(img, axis=-1).astype('uint16'),\n",
    "        skimage.morphology.disk(5))\n",
    "    entropy = entropy / float(img.size)\n",
    "    entropy_sum = np.sum(entropy)\n",
    "    entropy_mean = np.mean(entropy)\n",
    "    entropy_var = np.var(entropy)\n",
    "\n",
    "    # concatenate all the features together\n",
    "    feature_vec = np.concatenate(\n",
    "        [mean, median, cov, [entropy_sum, entropy_mean, entropy_var]])\n",
    "\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_image_categories(images):\n",
    "    \"\"\"Get the true categories of a set of paths to images, based on the\n",
    "    directory they are located in.\n",
    "\n",
    "    The paths should have the form:\n",
    "        path/to/image/category/image.jpg\n",
    "\n",
    "    Where the image filename is the last item in the path, and the\n",
    "    directory (category name) is the second to last item in the path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : list\n",
    "        List of paths to images\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    categories : numpy.ndarray\n",
    "        An array of integers in order of the images, corresponding to\n",
    "        each image's category\n",
    "    category_map : list\n",
    "        A list of category names. The category integers in\n",
    "        `categories` are indices into this list.\n",
    "\n",
    "    \"\"\"\n",
    "    get_category = lambda x: os.path.split(os.path.split(x)[0])[1]\n",
    "    categories = map(get_category, images)\n",
    "    category_map = sorted(set(categories))\n",
    "    categories = np.array(map(category_map.index, categories))\n",
    "    return categories, category_map\n",
    "\n",
    "\n",
    "def load_and_extract(images):\n",
    "    # placeholder variable for feature array\n",
    "    features = None\n",
    "\n",
    "    # go through each image and calculate features, saving them in the\n",
    "    # feature array\n",
    "    for i, image_path in enumerate(images):\n",
    "        # display progress\n",
    "        msg = \"[%d / %d] %s\" % (i, len(images), image_path)\n",
    "        sys.stdout.write(msg + \"\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # load and extract features\n",
    "        img = load_image(image_path)\n",
    "        img_features = extract_features(img)\n",
    "        if i == 0:\n",
    "            features = np.empty((len(images), img_features.size), dtype='f4')\n",
    "            sys.stdout.write(\" \"*len(msg) + \"\\r\")\n",
    "            print(\"Feature array has shape %s\" % str(features.shape))\n",
    "        features[i] = img_features\n",
    "\n",
    "        # clear the output (the \\r moves the cursor back to the\n",
    "        # beginning of the line, so we can overwrite it)\n",
    "        sys.stdout.write(\" \"*len(msg) + \"\\r\")\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # get the list of images\n",
    "    images = glob(\"./50_categories/*/*.jpg\")\n",
    "\n",
    "    # compute feature matrix\n",
    "    features = load_and_extract(images)\n",
    "\n",
    "    # create an integer mapping to categories\n",
    "    categories, category_map = get_image_categories(images)\n",
    "\n",
    "    # concatenate the features (X) and categories (Y)\n",
    "    dataset = np.hstack([features, categories[:, None]])\n",
    "\n",
    "    # save to disk\n",
    "    filename = \"./image_dataset.npy\"\n",
    "    np.save(filename, dataset)\n",
    "    print(\"Saved features to '%s'\" % filename)\n",
    "\n",
    "    filename = \"./image_categories.npy\"\n",
    "    np.save(filename, category_map)\n",
    "    print(\"Saved categories to '%s'\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/kevinli/src/Python_For_Data_Science/Kevin/Homework4/50_categories/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-28a6aee8320c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMYDIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mimage_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMYDIRECTORY\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mimage_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMYDIRECTORY\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/kevinli/src/Python_For_Data_Science/Kevin/Homework4/50_categories/.DS_Store'"
     ]
    }
   ],
   "source": [
    "# %load hw_4-machine-learning-parallel-strawman.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "AY 250 - Scientific Research Computing with Python\n",
    "Homework Assignment 4 - Parallel Feature Extraction Example\n",
    "Author: Christopher Klein, Joshua Bloom\n",
    "\"\"\"\n",
    "from os import listdir\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pylab import imread\n",
    "from time import time\n",
    "\n",
    "## CHANGE THIS NEXT LINE!\n",
    "#MYDIRECTORY = \"/Users/jbloom/Classes/ay250-py4sci/week4/50_categories\"\n",
    "MYDIRECTORY = \"/Users/kevinli/src/Python_For_Data_Science/Kevin/Homework4/50_categories\"\n",
    "# FUNCTION DEFINITIONS\n",
    "# Quick function to divide up a large list into multiple small lists, \n",
    "# attempting to keep them all the same size. \n",
    "def split_seq(seq, size):\n",
    "        newseq = []\n",
    "        splitsize = 1.0/size*len(seq)\n",
    "        for i in range(size):\n",
    "            newseq.append(seq[int(round(i*splitsize)):\n",
    "                int(round((i+1)*splitsize))])\n",
    "        return newseq\n",
    "# Our simple feature extraction function. It takes in a list of image paths, \n",
    "# does some measurement on each image, then returns a list of the image paths\n",
    "# paired with the results of the feature measurement.\n",
    "def extract_features(image_path_list):\n",
    "    feature_list = []\n",
    "    for image_path in image_path_list:\n",
    "        image_array = imread(image_path)\n",
    "        feature = image_array.size # This feature is simple. You can modify this\n",
    "        # code to produce more complicated features and to produce multiple\n",
    "        # features in one function call.\n",
    "        feature_list.append([image_path, feature])\n",
    "    return feature_list\n",
    "\n",
    "\n",
    "\n",
    "### Main program starts here ###################################################\n",
    "# We first collect all the local paths to all the images in one list\n",
    "image_paths = []\n",
    "categories = listdir(MYDIRECTORY)\n",
    "for category in categories:\n",
    "    image_names = listdir(MYDIRECTORY  + \"/\" + category)\n",
    "    for name in image_names:\n",
    "        image_paths.append(MYDIRECTORY + \"/\" + category + \"/\" + name)\n",
    "\n",
    "print (\"There should be 4244 images, actual number is \" + \n",
    "    str(len(image_paths)) + \".\")\n",
    "\n",
    "# Then, we run the feature extraction function using multiprocessing.Pool so \n",
    "# so that we can parallelize the process and run it much faster.\n",
    "numprocessors = cpu_count() # To see results of parallelizing, set numprocessors\n",
    "                            # to less than cpu_count().\n",
    "# numprocessors = 1\n",
    "\n",
    "# We have to cut up the image_paths list into the number of processes we want to\n",
    "# run. \n",
    "split_image_paths = split_seq(image_paths, numprocessors)\n",
    "\n",
    "# Ok, this block is where the parallel code runs. We time it so we can get a \n",
    "# feel for the speed up.\n",
    "start_time = time()\n",
    "p = Pool(numprocessors)\n",
    "result = p.map_async(extract_features, split_image_paths)\n",
    "poolresult = result.get()\n",
    "end_time = time()\n",
    "\n",
    "# All done, print timing results.\n",
    "print (\"Finished extracting features. Total time: \" + \n",
    "    str(round(end_time-start_time, 3)) + \" s, or \" + \n",
    "    str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
    "# This took about 10-11 seconds on my 2.2 GHz, Core i7 MacBook Pro. It may also\n",
    "# be affected by hard disk read speeds.\n",
    "\n",
    "# To tidy-up a bit, we loop through the poolresult to create a final list of\n",
    "# the feature extraction results for all images.\n",
    "combined_result = []\n",
    "for single_proc_result in poolresult:\n",
    "    for single_image_result in single_proc_result:\n",
    "        combined_result.append(single_image_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
