{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "from glob import glob\n",
    "import math\n",
    "from scipy import ndimage\n",
    "import skimage as ski\n",
    "from skimage.feature import hog\n",
    "from os import listdir\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pylab import imread\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train_folder='train/'\n",
    "test_folder='test/'\n",
    "\n",
    "WIDTH = 256\n",
    "HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_list = glob('./50_categories/*/*.jpg')\n",
    "cat_list = glob('./50_categories/*')\n",
    "categories = []\n",
    "cat_class = {}\n",
    "class_num = 0\n",
    "for cat in cat_list:\n",
    "    cat_name = cat.split(\"\\\\\")[1]\n",
    "    if not cat in categories:\n",
    "        categories.append(cat_name)\n",
    "        cat_class[cat_name] = class_num\n",
    "        class_num = class_num + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exist!\n"
     ]
    }
   ],
   "source": [
    "if not (os.path.exists(train_folder) and os.path.exists(test_folder) ) :\n",
    "    os.makedirs(train_folder)\n",
    "    os.makedirs(test_folder)\n",
    "    for im in image_list:\n",
    "        # 20% test data\n",
    "        in_train = random.random();\n",
    "        #resize and save in different folder\n",
    "        im_file = im.split(\"\\\\\")[2]\n",
    "        im_cat = im.split(\"\\\\\")[1]\n",
    "        im_class = cat_class[im_file.split(\"_\")[0]] \n",
    "        if in_train < 0.8 :\n",
    "            im_dest_file = train_folder + im_file\n",
    "            with open(train_folder+\"train.txt\", \"a\") as f:\n",
    "                f.write(im_file+\"\\t\"+str(im_class)+\"\\n\")\n",
    "        else:\n",
    "            im_dest_file = test_folder + im_file\n",
    "            with open(test_folder+\"test.txt\", \"a\") as f:\n",
    "                f.write(im_file+\"\\t\"+str(im_class)+\"\\n\")\n",
    "        img = cv2.imread(im)\n",
    "        img_res = cv2.resize(img,(WIDTH, HEIGHT), interpolation = cv2.INTER_AREA)\n",
    "        cv2.imwrite(im_dest_file, img_res)\n",
    "else:\n",
    "    print(\"Data already exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(image_path_list):\n",
    "    feature_list = []\n",
    "    for image_path in image_path_list:\n",
    "        img= imread(image_path)\n",
    "        RGB = img.reshape((-1, 3)).T\n",
    "\n",
    "        # mean of each channel\n",
    "        mean = np.mean(RGB, axis=1)\n",
    "        # median of each channel\n",
    "        median = np.median(RGB, axis=1)\n",
    "        # covariance between channels\n",
    "        cov = np.cov(RGB).ravel()\n",
    "        # (normalized) entropy of the grayscale image\n",
    "        entropy = ski.filters.rank.entropy(\n",
    "            np.mean(img, axis=-1).astype('uint16'),\n",
    "            ski.morphology.disk(5))\n",
    "        entropy = entropy / float(img.size)\n",
    "        entropy_sum = np.sum(entropy)\n",
    "        entropy_mean = np.mean(entropy)\n",
    "        entropy_var = np.var(entropy)\n",
    "    \n",
    "        #hog :http://scikit-image.org/docs/dev/auto_examples/plot_hog.html\n",
    "        img_grey = ski.color.rgb2gray(img)\n",
    "        fd, hog_image = hog(img_grey, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualise=True)\n",
    "        # concatenate all the features together\n",
    "        feature_vec = np.concatenate(\n",
    "            [mean, median, cov, [entropy_sum, entropy_mean, entropy_var], fd])\n",
    "        # code to produce more complicated features and to produce multiple\n",
    "        # features in one function call.\n",
    "        feature_list.append([image_path, feature_vec])\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_seq(seq, size):\n",
    "        newseq = []\n",
    "        splitsize = 1.0/size*len(seq)\n",
    "        for i in range(size):\n",
    "            newseq.append(seq[int(round(i*splitsize)):\n",
    "                int(round((i+1)*splitsize))])\n",
    "        return newseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = train_folder + 'train.txt'\n",
    "test_file = test_folder + 'test.txt'\n",
    "\n",
    "image_paths_train = []\n",
    "ymap = {}\n",
    "with open(train_file) as f:\n",
    "    for line in f:\n",
    "        train_img_file = line.split('\\t')[0]\n",
    "        class_num = line.split('\\t')[1]\n",
    "        image_paths_train.append(train_folder + train_img_file)\n",
    "        y[train_folder + train_img_file] = int(class_num)\n",
    "        \n",
    "image_paths_test = []\n",
    "ytmap = {}\n",
    "with open(test_file) as f:\n",
    "    for line in f:\n",
    "        test_img_file = line.split('\\t')[0]\n",
    "        class_num = line.split('\\t')[1]\n",
    "        image_paths_test.append(test_folder + test_img_file)\n",
    "        yt[test_folder + test_img_file] = int(class_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numprocessors = cpu_count()\n",
    "\n",
    "# train data\n",
    "\n",
    "split_image_paths_train = split_seq(image_paths_train, numprocessors)\n",
    "\n",
    "# Ok, this block is where the parallel code runs. We time it so we can get a \n",
    "# feel for the speed up.\n",
    "start_time = time()\n",
    "p = Pool(numprocessors)\n",
    "result_train = p.map_async(extract_features, split_image_paths_train)\n",
    "poolresult_train = result_train.get()\n",
    "end_time = time()\n",
    "\n",
    "# All done, print timing results.\n",
    "print (\"Finished extracting features. Total time: \" + \n",
    "    str(round(end_time-start_time, 3)) + \" s, or \" + \n",
    "    str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
    "\n",
    "combined_result_train = []\n",
    "for single_proc_result in poolresult_train:\n",
    "    for single_image_result in single_proc_result:\n",
    "        combined_result_train.append(single_image_result)\n",
    "\n",
    "#test data\n",
    "split_image_paths_test= split_seq(image_paths_test, numprocessors)\n",
    "\n",
    "# Ok, this block is where the parallel code runs. We time it so we can get a \n",
    "# feel for the speed up.\n",
    "start_time = time()\n",
    "p = Pool(numprocessors)\n",
    "result_test = p.map_async(extract_features, split_image_paths_test)\n",
    "poolresult_test = result_test.get()\n",
    "end_time = time()\n",
    "\n",
    "# All done, print timing results.\n",
    "print (\"Finished extracting features. Total time: \" + \n",
    "    str(round(end_time-start_time, 3)) + \" s, or \" + \n",
    "    str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
    "\n",
    "combined_result_test = []\n",
    "for single_proc_result in poolresult_test:\n",
    "    for single_image_result in single_proc_result:\n",
    "        combined_result_test.append(single_image_result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "#random forest not \n",
    "rf = RandomForestClassifier(n_estimators=i, n_jobs=2,oob_score=True)\n",
    "rf.fit(X, y)\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(x.shape[1]-1):\n",
    "    if (importances[indices[f]])>0.01:\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "oob_error = 1 - rf.oob_score_\n",
    "\n",
    "yts=rf.predict(XT)\n",
    "ets= 1- sum(yts==yt)/len(yt)\n",
    "F1=metrics.f1_score(yts,yt)\n",
    "print('%d %5.3f %5.3f %5.3f' % (i,oob_error,ets,F1))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
